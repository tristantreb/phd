{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will model the factor that interconnects the airway resistance between two consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.models.var_builders as var_builders\n",
    "import src.data.helpers as dh\n",
    "import src.data.breathe_data as bd\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import src.models.helpers as mh\n",
    "import src.models.cpts.helpers as cpth\n",
    "import src.modelling_ar.ar as model_ar\n",
    "import src.inference.long_inf_slicing as slicing\n",
    "import src.models.builders as mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    HFEV1,\n",
    "    ecFEV1,\n",
    "    AR,\n",
    "    HO2Sat,\n",
    "    O2SatFFA,\n",
    "    IA,\n",
    "    UO2Sat,\n",
    "    O2Sat,\n",
    "    ecFEF2575prctecFEV1,\n",
    ") = var_builders.o2sat_fev1_fef2575_point_in_time_model_shared_healthy_vars(\n",
    "    180, 10, \"Male\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dh.load_excel(\n",
    "    # f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_ecFEV1_ecFEF2575.xlsx\",\n",
    "    # f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_FEV1.xlsx\",\n",
    "    f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_ecFEV1_ecFEF2575.xlsx\",\n",
    "    [AR.name],\n",
    "    [\"Day\"],\n",
    "    # ).drop(columns=[\"Unnamed: 0\", HO2Sat.name, IA.name, HFEV1.name])\n",
    ").drop(columns=[HO2Sat.name, IA.name, HFEV1.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_elapsed_for_offset(df_for_ID, idx_offset=1):\n",
    "    \"\"\"\n",
    "    Links each recording with a previous recording that is idx_offset indices before it\n",
    "    For idx_offset = 1, consecutive recordings are linked\n",
    "    \"\"\"\n",
    "    df_for_ID = df_for_ID.copy()\n",
    "\n",
    "    def calc_days_elapsed(curr, prev):\n",
    "        \"\"\"\n",
    "        Takes in dates in format\n",
    "        \"\"\"\n",
    "        if prev == None:\n",
    "            return None\n",
    "        return (curr - prev).days\n",
    "        # return (curr - prev).total_seconds() / 3600 / 24\n",
    "\n",
    "    df_for_ID[\"Prev date\"] = df_for_ID.shift(idx_offset)[\"Day\"]\n",
    "    s_days_elapsed = df_for_ID.apply(\n",
    "        lambda x: calc_days_elapsed(x[\"Day\"], x[\"Prev date\"]), axis=1\n",
    "    )\n",
    "\n",
    "    return s_days_elapsed\n",
    "\n",
    "\n",
    "def get_days_elapsed_and_AR_mean_shift(df_for_ID, idx_offset=1):\n",
    "    df_for_ID = df_for_ID.copy()\n",
    "    df_for_ID[\"Days elapsed\"] = get_days_elapsed_for_offset(df_for_ID, idx_offset)\n",
    "\n",
    "    df_for_ID[\"AR mean\"] = df_for_ID.apply(lambda x: AR.get_mean(x[AR.name]), axis=1)\n",
    "    # df_for_ID['AR skewness'] = df_for_ID.apply(lambda x: AR.get_skewness(x[AR.name]), axis=1)\n",
    "\n",
    "    df_for_ID[\"Prev AR mean\"] = df_for_ID.shift(idx_offset)[\"AR mean\"]\n",
    "    # df_for_ID['Prev AR skewness'] = df_for_ID.shift(idx_offset)['AR skewness']\n",
    "\n",
    "    df_for_ID[\"AR mean shift\"] = df_for_ID[\"AR mean\"] - df_for_ID[\"Prev AR mean\"]\n",
    "    # df_for_ID['AR skewness shift'] = df_for_ID['AR skewness'] - df_for_ID['Prev AR skewness']\n",
    "\n",
    "    return df_for_ID[[\"ID\", \"Day\", \"Days elapsed\", \"AR mean shift\"]]\n",
    "    # return df_for_ID[['ID', 'Day', 'Days elapsed', 'AR mean shift', 'AR skewness shift']]\n",
    "\n",
    "\n",
    "def generate_AR_change_sample(df_samples_for_ID, idx_offset=1):\n",
    "    \"\"\"\n",
    "    1. Sample from AR1 and AR2\n",
    "    2. Compute the change in AR and save it\n",
    "    3. Repeat 500 times for this ID, then aggregate the results across IDs\n",
    "    500*300 ID = 150,000 samples, can do more if needed\n",
    "    \"\"\"\n",
    "    df_samples_for_ID = df_samples_for_ID.copy()\n",
    "\n",
    "    df_samples_for_ID[\"Days elapsed\"] = get_days_elapsed_for_offset(\n",
    "        df_samples_for_ID, idx_offset\n",
    "    )\n",
    "\n",
    "    df_samples_for_ID[\"Prev AR samples\"] = df_samples_for_ID.shift(idx_offset)[\n",
    "        \"AR samples\"\n",
    "    ]\n",
    "\n",
    "    # Remove entries at the boundaries that have no previous recordings after applying the offset\n",
    "    df_samples_for_ID = df_samples_for_ID.dropna(subset=[\"Prev AR samples\"])\n",
    "\n",
    "    df_samples_for_ID[\"AR samples shift\"] = df_samples_for_ID.apply(\n",
    "        lambda row: row[\"AR samples\"] - row[\"Prev AR samples\"], axis=1\n",
    "    )\n",
    "    df_samples_for_ID = df_samples_for_ID.explode(\"AR samples shift\")\n",
    "\n",
    "    return df_samples_for_ID[[\"ID\", \"Day\", \"Days elapsed\", \"AR samples shift\"]]\n",
    "\n",
    "\n",
    "# out = df.groupby('ID').apply(get_days_elapsed_and_AR_mean_shift).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute day elapsed between two consecutive entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.merge(\n",
    "    df.groupby(\"ID\").apply(get_days_elapsed_and_AR_mean_shift).reset_index(drop=True),\n",
    "    on=[\"ID\", \"Day\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Days elapsed\"] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the prev day is indeed correct)\n",
    "df1.iloc[2295:2297]\n",
    "# Count number of None\n",
    "print(df1[\"Days elapsed\"].isna().sum())\n",
    "# Count number if ids\n",
    "print(df1[\"ID\"].nunique())\n",
    "# They should be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"AR mean shift\"] > 20]\n",
    "df1.iloc[2537:2539]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse time between two consecutive entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df1[\"Days elapsed\"].value_counts()\n",
    "# 1/3 of the consecutive indices are more than 1 day apart (~10k entries)\n",
    "# 97% of the entries are less than 5 days apart from the previous entry\n",
    "# For the CPT, I'll take 1, 2, 3, 4, 5 days apart, then avg 6-50 -> this last up to the max days diff\n",
    "\n",
    "\n",
    "# Plot the histogram with vc index and vc values\n",
    "fig = px.bar(x=vc.index, y=vc.values / sum(vc.values) * 100)\n",
    "# Set x axis label to day to day difference\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Number of days between two consecutive entries\",\n",
    "    range=[0, 30],\n",
    "    tickvals=list(range(0, 31, 1)),\n",
    ")\n",
    "# Set y axis label to percentage\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Percentage of total entries (%)\", tickvals=[2] + list(range(0, 55, 5))\n",
    ")\n",
    "\n",
    "title = \"Distribution of the time between two measurements\"\n",
    "# Set title\n",
    "fig.update_layout(title=title, width=800, height=350, font=dict(size=10))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# # Save figure\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study per ID\n",
    "# Get idx at which the days elapsed is more than 3\n",
    "\n",
    "df1[df1[\"Days elapsed\"] > 3].index\n",
    "\n",
    "\n",
    "def get_idx_more_than_n_days_elapsed(df, n=3):\n",
    "    df = df.reset_index()\n",
    "    n_days_total = df.shape[0]\n",
    "    df_tmp = df[df[\"Days elapsed\"] > n]\n",
    "    if df_tmp.empty:\n",
    "        return n_days_total, n_days_total\n",
    "    n_days_consec = df_tmp.index[0]\n",
    "    return n_days_consec, n_days_total\n",
    "\n",
    "\n",
    "s_n_entries_to_break = (\n",
    "    df1.groupby(\"ID\")\n",
    "    .apply(lambda x: get_idx_more_than_n_days_elapsed(x, 3))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "s_n_entries_to_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.ID == \"101\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shift in airway resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max offset between entries will be equal to the max number of days elapsed in the model, to maximise the contributing data\n",
    "max_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AR norm\"] = df.apply(lambda row: row[AR.name] / sum(row[AR.name]), axis=1)\n",
    "df[\"AR samples\"] = df.apply(lambda row: AR.sample(n=50, p=row[\"AR norm\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/v2r6yn111s3gpdf8lzf72xvw0000gn/T/ipykernel_30041/2329533131.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(lambda df_for_ID: generate_AR_change_sample(df_for_ID, n_idx_offset))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/v2r6yn111s3gpdf8lzf72xvw0000gn/T/ipykernel_30041/2329533131.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(lambda df_for_ID: generate_AR_change_sample(df_for_ID, n_idx_offset))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zq/v2r6yn111s3gpdf8lzf72xvw0000gn/T/ipykernel_30041/2329533131.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(lambda df_for_ID: generate_AR_change_sample(df_for_ID, n_idx_offset))\n"
     ]
    }
   ],
   "source": [
    "# Build aggregate df of shift in AR for different offsets\n",
    "\n",
    "df_mixed_offset = pd.DataFrame()\n",
    "\n",
    "for n_idx_offset in range(1, max_offset + 1):\n",
    "    print(\"offset\", n_idx_offset)\n",
    "    df_offset = (\n",
    "        df.groupby(\"ID\")\n",
    "        .apply(lambda df_for_ID: generate_AR_change_sample(df_for_ID, n_idx_offset))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_offset[\"Offset\"] = n_idx_offset\n",
    "    # Remove nan\n",
    "    df_offset = df_offset.dropna()\n",
    "\n",
    "    # Add to mix offset\n",
    "    df_mixed_offset = pd.concat([df_mixed_offset, df_offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Day</th>\n",
       "      <th>Days elapsed</th>\n",
       "      <th>AR samples shift</th>\n",
       "      <th>Offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.525903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.065192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.344994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.113095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>2019-01-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.67779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013445</th>\n",
       "      <td>553</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-5.272935</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013446</th>\n",
       "      <td>553</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.47702</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013447</th>\n",
       "      <td>553</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-3.613054</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013448</th>\n",
       "      <td>553</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-2.605032</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013449</th>\n",
       "      <td>553</td>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.990397</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6087950 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID         Day  Days elapsed AR samples shift  Offset\n",
       "0        101  2019-01-26           1.0        -1.525903       1\n",
       "1        101  2019-01-26           1.0         6.065192       1\n",
       "2        101  2019-01-26           1.0         2.344994       1\n",
       "3        101  2019-01-26           1.0        -2.113095       1\n",
       "4        101  2019-01-26           1.0         -6.67779       1\n",
       "...      ...         ...           ...              ...     ...\n",
       "2013445  553  2023-11-10          30.0        -5.272935       3\n",
       "2013446  553  2023-11-10          30.0         -0.47702       3\n",
       "2013447  553  2023-11-10          30.0        -3.613054       3\n",
       "2013448  553  2023-11-10          30.0        -2.605032       3\n",
       "2013449  553  2023-11-10          30.0         5.990397       3\n",
       "\n",
       "[6087950 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mixed_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the shift in airway resistance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:* Checking for same day measurements *\n"
     ]
    }
   ],
   "source": [
    "# The previous methods was erroneous because it was taking to independent samples from AR day 1 and AR day 2.\n",
    "# We have to generate joint samples. Given a two days, model,\n",
    "# 1/ Infer AR1 using the two consecutive days model\n",
    "# 2/ Sample from AR1\n",
    "# 3/ Infer HFEV1 and HO2Sat using the sampled AR1 and the observations\n",
    "# 4/ Infer AR2 using the inferred HFEV1 and HO2Sat\n",
    "# 5/ Sample from AR2\n",
    "df_obs = bd.load_meas_from_excel(\"BR_O2_FEV1_FEF2575_conservative_smoothing_with_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_jointly_from_AR(df_two_days, date_1, date_2):\n",
    "    df_two_days = df_two_days.copy().reset_index(drop=True)\n",
    "    height = df_two_days.loc[0, \"Height\"]\n",
    "    age = df_two_days.loc[0, \"Age\"]\n",
    "    sex = df_two_days.loc[0, \"Sex\"]\n",
    "    id = df_two_days.loc[0, \"ID\"]\n",
    "    (\n",
    "        model,\n",
    "        inf_alg,\n",
    "        HFEV1,\n",
    "        ecFEV1,\n",
    "        AR,\n",
    "        HO2Sat,\n",
    "        O2SatFFA,\n",
    "        IA,\n",
    "        UO2Sat,\n",
    "        O2Sat,\n",
    "        ecFEF2575prctecFEV1,\n",
    "    ) = mb.o2sat_fev1_fef2575_point_in_time_model_shared_healthy_vars(height, age, sex)\n",
    "\n",
    "    # Set variables parametrisation\n",
    "    key_hfev1 = f\"['{ecFEV1.name}', '{HFEV1.name}', '{AR.name}'] -> {HFEV1.name}\"\n",
    "    key_ho2sat = f\"['{O2SatFFA.name}', '{HO2Sat.name}', '{AR.name}'] -> {HO2Sat.name}\"\n",
    "    HFEV1.set_factor_node_key(key_hfev1)\n",
    "    HO2Sat.set_factor_node_key(key_ho2sat)\n",
    "\n",
    "    # 1/ Infer AR1 using the two consecutive days model\n",
    "    df_res_final_epoch1, _, _ = slicing.query_back_and_forth_across_days_joint_samples(\n",
    "        df_two_days,\n",
    "        inf_alg,\n",
    "        [HFEV1, HO2Sat],\n",
    "        [AR],\n",
    "        [ecFEV1.name, ecFEF2575prctecFEV1.name],\n",
    "        1e-8,\n",
    "        days_specific_evidence=[],\n",
    "        max_passes=5,\n",
    "    )\n",
    "\n",
    "    df_res_final_epoch1.set_index(\"Day\", inplace=True)\n",
    "\n",
    "    # 2/ Sample from AR1\n",
    "    ar_day1_dist = df_res_final_epoch1.loc[date_1, AR.name]\n",
    "    [ar_day1_sample] = AR.sample(n=1, p=ar_day1_dist)\n",
    "    idx_ar = AR.get_bin_for_value(ar_day1_sample)[1]\n",
    "\n",
    "    v = np.zeros(len(df_two_days)) - 1\n",
    "    v[0] = ar_day1_sample\n",
    "    df_two_days[\"AR\"] = v\n",
    "\n",
    "    v = np.zeros(len(df_two_days)) + 10000\n",
    "    v[0] = idx_ar\n",
    "    v = v.astype(int)\n",
    "    df_two_days[f\"idx {AR.name}\"] = v\n",
    "\n",
    "    # 3/ Infer AR2 using with sampled AR1 as evidence specific to day 1\n",
    "    days_specific_evidence = [(AR.name, [date_1])]\n",
    "\n",
    "    df_res_final_epoch2, _, _ = slicing.query_back_and_forth_across_days_joint_samples(\n",
    "        df_two_days,\n",
    "        inf_alg,\n",
    "        [HFEV1, HO2Sat],\n",
    "        [AR],\n",
    "        [ecFEV1.name, ecFEF2575prctecFEV1.name],\n",
    "        1e-8,\n",
    "        days_specific_evidence,\n",
    "        max_passes=5,\n",
    "        debug=False,\n",
    "    )\n",
    "    df_res_final_epoch2.set_index(\"Day\", inplace=True)\n",
    "    ar_day2_dist = df_res_final_epoch2.loc[date_2, AR.name]\n",
    "\n",
    "    # Print the interquartile ranges of the AR distributions\n",
    "    # ar1_1 = AR.get_val_at_quantile(ar_day1_dist, 0.25)\n",
    "    # ar1_2 = AR.get_val_at_quantile(ar_day1_dist, 0.75)\n",
    "    # print(f\"AR1: {ar1_2} - {ar1_1} = {ar1_2 - ar1_1}\")\n",
    "\n",
    "    # ar2_1 = AR.get_val_at_quantile(ar_day2_dist, 0.25)\n",
    "    # ar2_2 = AR.get_val_at_quantile(ar_day2_dist, 0.75)\n",
    "    # print(f\"AR2: {ar2_2} - {ar2_1} = {ar2_2 - ar2_1}\")\n",
    "\n",
    "    # 5/ Sample from AR2\n",
    "    [ar_day2_sample] = AR.sample(n=1, p=ar_day2_dist)\n",
    "\n",
    "    ar_shift = ar_day2_sample - ar_day1_sample\n",
    "\n",
    "    return ar_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1576"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_max_ecFEV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 527, N: 5\n",
      "ID: 527, offset: 1\n",
      "** Max ecFEV1 in the two days, skipping **\n",
      "ID: 527, idx: 4, offset: 1, breaking\n",
      "ID: 527, offset: 2\n",
      "** Max ecFEV1 in the two days, skipping **\n",
      "ID: 527, idx: 3, offset: 2, breaking\n",
      "ID: 527, offset: 3\n",
      "** Max ecFEV1 in the two days, skipping **\n",
      "ID: 527, idx: 2, offset: 3, breaking\n"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "\n",
    "max_offset = 3\n",
    "\n",
    "for id in [\"527\"]:  # df_obs[\"ID\"][0:1].unique():\n",
    "    print(f\"ID: {id}, N: {len(df_obs[df_obs['ID'] == id])}\")\n",
    "    df_for_ID = df_obs[df_obs[\"ID\"] == id].reset_index(drop=True)\n",
    "\n",
    "    for n_idx_offset in list(np.arange(1, max_offset + 1)):\n",
    "        print(f\"ID: {id}, offset: {n_idx_offset}\")\n",
    "\n",
    "        for i, row in df_for_ID.iterrows():\n",
    "            if i + n_idx_offset >= len(df_for_ID):\n",
    "                print(f\"ID: {id}, idx: {i}, offset: {n_idx_offset}, breaking\")\n",
    "                break\n",
    "            # print(f\"ID: {id}, idx: {i}, offset: {n_idx_offset}\")\n",
    "            # Find idx of max ecFEV1\n",
    "            idx_max_ecFEV1 = idx_max_FEV1 = df_for_ID.sort_values(\n",
    "                by=[\"ecFEV1\", \"ecFEF2575\", \"O2 Saturation\"], ascending=False\n",
    "            ).index[0]\n",
    "            # Get two first days as well as idx_max_ecFEV1\n",
    "            idx_two_days = [i, i + n_idx_offset]\n",
    "            if idx_max_ecFEV1 in idx_two_days:\n",
    "                print(\"** Max ecFEV1 in the two days, skipping **\")\n",
    "                continue\n",
    "\n",
    "            # df_two_days = df_for_ID.iloc[idx_two_days]\n",
    "            # Check that IQR reduces when adding the max ecFEV1: use ID 134\n",
    "            df_two_days = df_for_ID.iloc[idx_two_days + [idx_max_ecFEV1]].reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            day_1 = df_two_days.loc[0, \"Date Recorded\"]\n",
    "            day_1_str = day_1.strftime(\"%Y-%m-%d\")\n",
    "            day_2 = df_two_days.loc[1, \"Date Recorded\"]\n",
    "            day_2_str = day_2.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            ar_shift = sample_jointly_from_AR(df_two_days, day_1_str, day_2_str)\n",
    "            days_elapsed = (day_2 - day_1).days\n",
    "\n",
    "            # Add row to table with format: ID, date, days elapsed, AR shift, offset\n",
    "            new_row = pd.DataFrame(\n",
    "                data=[\n",
    "                    [\n",
    "                        df_two_days.loc[0, \"ID\"],\n",
    "                        df_two_days.loc[0, \"Date Recorded\"],\n",
    "                        days_elapsed,\n",
    "                        ar_shift,\n",
    "                        n_idx_offset,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"ID\",\n",
    "                    \"Date Recorded\",\n",
    "                    \"Days elapsed\",\n",
    "                    \"AR samples shift\",\n",
    "                    \"Offset\",\n",
    "                ],\n",
    "            )\n",
    "            res = pd.concat([res, new_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date Recorded</th>\n",
       "      <th>Days elapsed</th>\n",
       "      <th>AR samples shift</th>\n",
       "      <th>Offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.701479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.707074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>1</td>\n",
       "      <td>3.231084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.957549</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.919641</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>3</td>\n",
       "      <td>2.018751</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID Date Recorded  Days elapsed  AR samples shift  Offset\n",
       "0  527    2022-05-24             1         -2.701479       1\n",
       "0  527    2022-05-25             1          1.707074       1\n",
       "0  527    2022-05-26             1          3.231084       1\n",
       "0  527    2022-05-24             2          0.957549       2\n",
       "0  527    2022-05-25             2          0.919641       2\n",
       "0  527    2022-05-24             3          2.018751       3"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with days elapsed on x axis and AR diff on y axis, using px\n",
    "y_col = \"AR mean shift\"\n",
    "y_col = \"AR samples shift\"\n",
    "# y_col = 'AR skewness shift'\n",
    "fig = px.scatter(df_mixed_offset, x=\"Days elapsed\", y=y_col, color=\"ID\")\n",
    "# Set x axis range to 0-100\n",
    "fig.update_xaxes(range=[0, 200])\n",
    "fig.update_xaxes(range=[0, 50], title=\"Number of days elapsed\")\n",
    "# Add more y axi tick vals\n",
    "fig.update_yaxes(title=\"Mean airway resistance shift (%)\")\n",
    "# Reduce marker size\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "title = f\"How much does the airway resistance change in n days (1- {max_offset} idx offset)? - samples\"\n",
    "fig.update_layout(\n",
    "    title=title, width=800, height=400, font=dict(size=10), showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see the distribution of AR diffs for each day elapsed\n",
    "from scipy.stats import norm\n",
    "\n",
    "y_col = \"AR mean shift\"\n",
    "y_col = \"AR samples shift\"\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=True)\n",
    "xbin_size = 0.2\n",
    "# xbin_size = 1\n",
    "xbin_absolute_span = 50\n",
    "# xbin_absolute_span = 10\n",
    "xbins = dict(\n",
    "    start=-xbin_absolute_span - 0.5, end=xbin_absolute_span + 0.5, size=xbin_size\n",
    ")\n",
    "\n",
    "\n",
    "def add_plot_for_n_days_elapsed(n_days_elapsed, row):\n",
    "    df_tmp = df_mixed_offset[df_mixed_offset[\"Days elapsed\"] == n_days_elapsed]\n",
    "    print(n_days_elapsed, df_tmp.shape)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df_tmp[y_col],\n",
    "            xbins=xbins,\n",
    "            histnorm=\"probability\",\n",
    "            name=(\n",
    "                f\"{n_days_elapsed} day elapsed\"\n",
    "                if n_days_elapsed == 1\n",
    "                else f\"{n_days_elapsed} days elapsed\"\n",
    "            ),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=1,\n",
    "    )\n",
    "    return df_tmp\n",
    "    # Model the data by a normal distribution\n",
    "    # mean = df_tmp[y_col].mean()\n",
    "    # std = df_tmp[y_col].std()\n",
    "    # x = list(range(-10, 11))\n",
    "    # y = norm.pdf(x, loc=mean, scale=std)\n",
    "    # Add trace\n",
    "    # fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=f\"Normal distribution for {offset} days offset\"), row=row, col=1)\n",
    "\n",
    "\n",
    "# for offset in range(1, 51):\n",
    "#     add_plot_for_n_days_elapsed(offset, offset)\n",
    "\n",
    "df_1_DE = add_plot_for_n_days_elapsed(1, 1)\n",
    "add_plot_for_n_days_elapsed(2, 2)\n",
    "add_plot_for_n_days_elapsed(8, 3)\n",
    "add_plot_for_n_days_elapsed(14, 4)\n",
    "add_plot_for_n_days_elapsed(20, 5)\n",
    "add_plot_for_n_days_elapsed(50, 6)\n",
    "\n",
    "# Set y axis range to 0, 0.6\n",
    "# fig.update_yaxes(range=[0, 0.58])\n",
    "# Set x axis label\n",
    "fig.update_xaxes(title_text=\"Shift in mean airway resistance (%)\", row=6, col=1)\n",
    "# fig.update_xaxes(title_text='Change in skewness of airway resistance (%)', row=6, col=1)\n",
    "# Add x axis tick vals\n",
    "# fig.update_xaxes(tickvals=np.arange(-10, 11, 1), row=6, col=1)\n",
    "# fig.update_xaxes(tickvals=np.arange(-50, 55, 5), row=6, col=1)\n",
    "# Update layout\n",
    "# title = f\"Shift in airway resistance for different time periods elapsed (bin_width = {xbin_size}%, bin_span = {xbin_absolute_span})\"\n",
    "title = f\"Shift in airway resistance - O2sat, ecFEV1\"\n",
    "# fig.update_layout(height=2600, width=1000, title=title)\n",
    "fig.update_layout(height=600, width=1000, title=title)\n",
    "\n",
    "# Keep y axis lower\n",
    "fig.update_yaxes(range=[0, 0.01])\n",
    "#\n",
    "# Save image\n",
    "fig.write_image(\n",
    "    f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\n",
    "    f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title} - samples.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building P(AR_next | days_elapsed, AR_prev)\n",
    "# import src.models.helpers as mh\n",
    "import numpy as np\n",
    "import src.modelling_ar.ar as model_ar\n",
    "\n",
    "AR1 = mh.VariableNode(\n",
    "    \"Airway resistance day 1 (%)\", 0, 90, 2, prior={\"type\": \"uniform\"}\n",
    ")\n",
    "AR2 = mh.VariableNode(\n",
    "    \"Airway resistance day 2 (%)\", 0, 90, 2, prior={\"type\": \"uniform\"}\n",
    ")\n",
    "# Set the max number of days elapsed to max offset in order to have as much data as possible per number of days elapsed\n",
    "DE = mh.DiscreteVariableNode(\"Days elapsed\", 1, max_offset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cpt(\n",
    "    AR_next_day: mh.VariableNode,\n",
    "    AR_curr_day: mh.VariableNode,\n",
    "    DE: mh.DiscreteVariableNode,\n",
    "    shift_p,\n",
    "    shift_val,\n",
    "    tol=1e-6,\n",
    "    debug=False,\n",
    "):\n",
    "    cpt = np.zeros([AR_next_day.card, AR_curr_day.card, DE.card])\n",
    "\n",
    "    for i, de in enumerate(DE.values):\n",
    "        # For each shift value, get the mapping AR -> AR_next_day for each shifted bin in AR\n",
    "        # Weight the result by the probability of that shift\n",
    "        # Add it to the CPT for this day\n",
    "        for s in range(len(shift_val)):\n",
    "            if debug:\n",
    "                print(f\"Computing CPT for days elapsed={de}, shift={shift_val[s]}\")\n",
    "            # Summing over the columns of the cpt returned by calc_cpt_X_plus_k should give 1, except at the boundaries\n",
    "            # Since we weight the 1s by a probability of shift that also sums to one, the sum of the cpt should be 1 (except at the boundaries, see below)\n",
    "            cpt_contrib = calc_cpt_X_plus_k(\n",
    "                AR_curr_day,\n",
    "                AR_next_day,\n",
    "                shift_val[s],\n",
    "                tol=tol,\n",
    "                debug=debug,\n",
    "            )\n",
    "            # If has nan\n",
    "            if (np.isnan(cpt_contrib) == True).any():\n",
    "                print(\"issue with cpt contribution\")\n",
    "                print(cpt_contrib)\n",
    "            cpt[:, :, i] += shift_p[i, s] * cpt_contrib\n",
    "        # Normalise the CPT along axis 0 (AR_next_day)\n",
    "        total = np.sum(cpt[:, :, i], axis=0)\n",
    "        if (np.isnan(total) == False).all():\n",
    "            print(cpt[:, :, i])\n",
    "        print(\n",
    "            f\"Sum along axis 0 before normalisation: np.sum(cpt[:, :, {i}], axis=0) = {total}\"\n",
    "        )\n",
    "        cpt[:, :, i] /= total\n",
    "\n",
    "        # Check that the sum of probabilities is 1\n",
    "        total = np.sum(cpt[:, :, i], axis=0)\n",
    "        assert (\n",
    "            abs(total - 1) < tol\n",
    "        ).all(), f\"The sum of the probabilities should be 1, got sum(cpt)={total}])\"\n",
    "    return cpt\n",
    "\n",
    "\n",
    "def calc_cpt_X_plus_k(\n",
    "    Z: mh.VariableNode,\n",
    "    X: mh.VariableNode,\n",
    "    k,\n",
    "    tol=1e-6,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the CPT for P(Z|X, Y), when Z is shifted from X by a constant value k\n",
    "    Z = X + k\n",
    "    X: parent variable\n",
    "    Z: child variable\n",
    "    k: constant, positive or negative\n",
    "\n",
    "    We compute the CPT with a shift and conquer method:\n",
    "    1) Start with a CPT zeroed out probabilities\n",
    "    2) Shift all X bin intervals by the drop amount\n",
    "    3) For each shifted X bin, spread the X bin evenly onto the overlapping Z bins\n",
    "    4) Normalise the CPT\n",
    "\n",
    "    This allows the function to be agnostic of how X and Z are binned.\n",
    "\n",
    "    - What happens when the function is shifted outside the boundary? -> Raise an error as it shouldn't happen by how the model is built\n",
    "    \"\"\"\n",
    "    nbinsX = len(X.bins)\n",
    "    nbinsZ = len(Z.bins)\n",
    "\n",
    "    cpt = np.zeros([nbinsZ, nbinsX])\n",
    "\n",
    "    for i in range(nbinsX):\n",
    "        shifted_X_bin_low = X.bins[i] + k\n",
    "        shifted_X_bin_up = (X.bins[i] + X.bin_width) + k\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"Shifting X bin {i} from [{X.bins[i]};{X.bins[i]+X.bin_width}) to [{shifted_X_bin_low};{shifted_X_bin_up}), shift amount={k}%\"\n",
    "            )\n",
    "        # If the shifted bin is outside the boundaries of Z, continue:\n",
    "        if (\n",
    "            shifted_X_bin_low >= (Z.bins[-1] + Z.bin_width)\n",
    "            or shifted_X_bin_up <= Z.bins[0]\n",
    "        ):\n",
    "            if debug:\n",
    "                print(\n",
    "                    f\"Shift outside boundaries of Z.bins=[{Z.bins[0]};{Z.bins[-1] + Z.bin_width})\"\n",
    "                )\n",
    "            continue\n",
    "        # Handle the case where the shifted bin is partially outside the boundaries\n",
    "        # Adjust the boundaries of the shifted bin to be within the boundaries of Z\n",
    "        if shifted_X_bin_low < Z.bins[0]:\n",
    "            if debug:\n",
    "                print(\"Shift partially outside boundaries, adjusting lower boundary\")\n",
    "            shifted_X_bin_low = Z.bins[0]\n",
    "        if shifted_X_bin_up > Z.bins[-1] + Z.bin_width:\n",
    "            if debug:\n",
    "                print(\"Shift partially outside boundaries, adjusting upper boundary\")\n",
    "            shifted_X_bin_up = Z.bins[-1] + Z.bin_width\n",
    "\n",
    "        bin_contribution = mh.get_bin_contribution_to_cpt(\n",
    "            [shifted_X_bin_low, shifted_X_bin_up], Z.bins, debug=debug\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"i={i}/{nbinsX-1}, z={bin_contribution}\")\n",
    "        # There is just one bin contribution to the CPT\n",
    "        cpt[:, i] = bin_contribution\n",
    "\n",
    "    sum_over_x = np.sum(cpt, axis=0)\n",
    "    if debug:\n",
    "        print(f\"Results before normalisation sum(cpt)={sum_over_x}\")\n",
    "\n",
    "    # IMPORTANT: there is no boundary check in this function. This allows to have no conditional probability distribution for certain bins of AR2, which are not compatible with the amount of shift applied to AR1\n",
    "    # Therefore either the sum of probabilities is 0 or 1 for each bin of AR2, summed over AR1\n",
    "    for i in range(nbinsZ):\n",
    "        if sum_over_x[i] == 0:\n",
    "            if debug:\n",
    "                print(f\"Sum of probabilities is 0 for bin {i}, skipping normalisation\")\n",
    "            continue\n",
    "        cpt[i, :] /= sum_over_x[i]\n",
    "        assert (\n",
    "            abs(sum_over_x[i] - 1) < tol\n",
    "        ).all(), f\"The sum of the probabilities should be 1, got sum(cpt[i, :])={sum_over_x[i]}])\"\n",
    "\n",
    "    return cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the shift distributions\n",
    "size = 0.2\n",
    "shift_min = -20\n",
    "shift_max = 20\n",
    "shift_val = np.arange(shift_min, shift_max + size / 2, 0.2)\n",
    "shift_p = np.empty((max_offset, len(shift_val)))\n",
    "\n",
    "# Check identity matrix if shift is 0\n",
    "# cpt_point_mass = np.zeros(len(shift_val))\n",
    "# cpt_point_mass[100] = 1\n",
    "\n",
    "for i, de in enumerate(DE.values):\n",
    "    print(\"days elapsed: \", de)\n",
    "    shift = df_mixed_offset[df_mixed_offset[\"Days elapsed\"] == de][\"AR samples shift\"]\n",
    "\n",
    "    # Bin up the mean shift series into bins starting at -5 and ending at 5, with bin size 1\n",
    "    shift_p[i, :] = np.histogram(\n",
    "        shift,\n",
    "        bins=np.arange(shift_min - size / 2, shift_max + size, size),\n",
    "        density=True,\n",
    "    )[0]\n",
    "    # shift_p[i, :] = cpt_point_mass\n",
    "\n",
    "print(\"shift probability shape: \", shift_p.shape)\n",
    "print(\"shift_val: \", shift_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = calc_cpt(AR2, AR1, DE, shift_p, shift_val, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test uniform shift distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: With the completely uniform CPT, each AR1 bin contributes equally to all AR2 bins, hence the output is uniform over AR2\n",
    "\n",
    "p_ar2 = np.ones(AR2.card) / AR2.card\n",
    "# Repeat p_ar2 on each column AR1.card times\n",
    "cpt_uni = np.repeat(p_ar2[:, np.newaxis], AR1.card, axis=1)\n",
    "# Repeat this cpt DE.card times\n",
    "cpt_uni = np.repeat(cpt_uni[:, :, np.newaxis], DE.card, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P of AR2 is uniform over 5 bins centered on the bin of AR1\n",
    "cpt_ar2_ar1 = np.zeros([AR2.card, AR1.card])\n",
    "# Add padding left and right\n",
    "padding = 15\n",
    "for i in range(AR1.card):\n",
    "    if i <= padding:\n",
    "        low = 0\n",
    "    else:\n",
    "        low = i - padding\n",
    "    if AR2.card - 1 <= (i + padding):\n",
    "        up = AR2.card - 1\n",
    "    else:\n",
    "        up = i + padding\n",
    "\n",
    "    idx_range = list(range(low, up + 1))\n",
    "    cpt_ar2_ar1[idx_range, i] = 1 / len(idx_range)\n",
    "\n",
    "cpt_thick_uni = np.repeat(cpt_ar2_ar1[:, :, np.newaxis], DE.card, axis=2)\n",
    "cpt_thick_uni.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CPT relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.inference.helpers as ih\n",
    "\n",
    "# cpt = cpt_uni\n",
    "\n",
    "\n",
    "def compare_ARs_for_one_entry(idx):\n",
    "    title = f\"P(AR_next | AR_prev, days_elapsed) for diffent days elapsed (idx {idx}) - samples\"\n",
    "    fig = make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    ih.plot_histogram(\n",
    "        fig, AR1, df.loc[idx, AR.name], AR1.a, AR1.b, 1, 1, name=\"AR day 1\", annot=False\n",
    "    )\n",
    "    AR_next_day_p = np.matmul(cpt[:, :, 0], df.loc[idx, AR.name])\n",
    "    ih.plot_histogram(\n",
    "        fig,\n",
    "        AR2,\n",
    "        AR_next_day_p,\n",
    "        AR2.a,\n",
    "        AR2.b,\n",
    "        1,\n",
    "        1,\n",
    "        name=\"AR day 2, days elapsed=1\",\n",
    "        annot=False,\n",
    "    )\n",
    "    # AR_next_day_p = np.matmul(cpt[:, :, 2], df.loc[idx, AR.name])\n",
    "    # ih.plot_histogram(\n",
    "    #     fig,\n",
    "    #     AR2,\n",
    "    #     AR_next_day_p,\n",
    "    #     AR2.a,\n",
    "    #     AR2.b,\n",
    "    #     1,\n",
    "    #     1,\n",
    "    #     name=\"AR day 2, days elapsed=3\",\n",
    "    #     annot=False,\n",
    "    # )\n",
    "    # Add x axis title\n",
    "    fig.update_xaxes(title_text=\"Airway resistance (%)\", row=1, col=1)\n",
    "    # Reduce figure height\n",
    "    fig.update_layout(height=200, width=1000, title=title, font=dict(size=10))\n",
    "    # remove marings\n",
    "    fig.update_layout(margin=dict(l=2, r=2, t=30, b=2))\n",
    "    fig.show()\n",
    "    # Save figure\n",
    "    # fig.write_image(\n",
    "    #     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "    # )\n",
    "\n",
    "\n",
    "compare_ARs_for_one_entry(20000)\n",
    "# compare_ARs_for_one_entry(21000)\n",
    "compare_ARs_for_one_entry(1000)\n",
    "compare_ARs_for_one_entry(4400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = 3\n",
    "fig, title = cpth.plot_2d_cpt(cpt_thick_uni[:, :, de - 1], AR2, AR1, 3000, invert=False)\n",
    "# Update font\n",
    "title = title + f\", {de} days elapsed, shift span [{shift_min};{shift_max}] - samples\"\n",
    "fig.update_layout(font=dict(size=7), title=title)\n",
    "fig.show()\n",
    "\n",
    "# Save figure\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cpt\n",
    "cpth.save_cpt([AR2, AR1, DE], cpt, suffix=f\"_shift_span_[{shift_min};{shift_max}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the shift per bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR.midbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df1.copy()\n",
    "\n",
    "for i, row in df_exploded[0:10].iterrows():\n",
    "    row = pd.DataFrame(data=row[AR.name])\n",
    "    df_exploded = pd.concat([df_exploded, row], axis=1)\n",
    "\n",
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
