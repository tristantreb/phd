{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will model the factor that interconnects the airway resistance between two consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.models.var_builders as var_builders\n",
    "import src.data.helpers as dh\n",
    "import src.data.breathe_data as bd\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import src.models.helpers as mh\n",
    "import src.models.cpts.helpers as cpth\n",
    "import src.modelling_ar.ar as model_ar\n",
    "import src.inference.long_inf_slicing as slicing\n",
    "import src.models.builders as mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    HFEV1,\n",
    "    ecFEV1,\n",
    "    AR,\n",
    "    HO2Sat,\n",
    "    O2SatFFA,\n",
    "    IA,\n",
    "    UO2Sat,\n",
    "    O2Sat,\n",
    "    ecFEF2575prctecFEV1,\n",
    ") = var_builders.o2sat_fev1_fef2575_point_in_time_model_shared_healthy_vars(\n",
    "    180, 10, \"Male\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_ecFEV1_ecFEF2575.xlsx\",\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_FEV1.xlsx\",\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_to_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_ecFEV1_ecFEF2575.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mAR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ).drop(columns=[\"Unnamed: 0\", HO2Sat.name, IA.name, HFEV1.name])\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[HO2Sat\u001b[38;5;241m.\u001b[39mname, IA\u001b[38;5;241m.\u001b[39mname, HFEV1\u001b[38;5;241m.\u001b[39mname])\n",
      "File \u001b[0;32m~/Desktop/PhD/Code/phd/src/data/helpers.py:35\u001b[0m, in \u001b[0;36mload_excel\u001b[0;34m(file_path, str_cols_to_arrays, date_cols)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_excel\u001b[39m(file_path, str_cols_to_arrays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, date_cols\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Load excel file\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Optionally convert string columns to arrays\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# If ID in columns set type to str\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/pandas/io/excel/_base.py:517\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 517\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/pandas/io/excel/_base.py:1629\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1591\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m   1610\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/pandas/io/excel/_base.py:793\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m    790\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m    792\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[0;32m--> 793\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/pandas/io/excel/_openpyxl.py:616\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    614\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    615\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[1;32m    617\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/worksheet/_read_only.py:79\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[0;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[1;32m     75\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source()\n\u001b[1;32m     76\u001b[0m parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[1;32m     77\u001b[0m                          data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[1;32m     78\u001b[0m                          date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m max_row:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:155\u001b[0m, in \u001b[0;36mparse\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:286\u001b[0m, in \u001b[0;36mopenpyxl.worksheet._reader.WorkSheetParser.parse_row\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:227\u001b[0m, in \u001b[0;36mopenpyxl.worksheet._reader.WorkSheetParser.parse_cell\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/descriptors/serialisable.py:103\u001b[0m, in \u001b[0;36mSerialisable.from_tree\u001b[0;34m(cls, node)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         attrib[tag] \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattrib\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/cell/text.py:167\u001b[0m, in \u001b[0;36mText.__init__\u001b[0;34m(self, t, r, rPh, phoneticPr)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m              t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    163\u001b[0m              r\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    164\u001b[0m              rPh\u001b[38;5;241m=\u001b[39m(),\n\u001b[1;32m    165\u001b[0m              phoneticPr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m             ):\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr \u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrPh \u001b[38;5;241m=\u001b[39m rPh\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/descriptors/nested.py:35\u001b[0m, in \u001b[0;36mNested.__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTag does not match attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_tree(value)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNested\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__set__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/descriptors/base.py:68\u001b[0m, in \u001b[0;36mConvertible.__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_none \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_none):\n\u001b[1;32m     67\u001b[0m     value \u001b[38;5;241m=\u001b[39m _convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_type, value)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConvertible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__set__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/phd/lib/python3.10/site-packages/openpyxl/descriptors/base.py:38\u001b[0m, in \u001b[0;36mTyped.__set__\u001b[0;34m(self, instance, value)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Typed, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues must be of type \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_type)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__set__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance, value):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_type):\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_none\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_none \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = dh.load_excel(\n",
    "    # f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_ecFEV1_ecFEF2575.xlsx\",\n",
    "    # f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_O2Sat_FEV1.xlsx\",\n",
    "    f\"{dh.get_path_to_main()}/ExcelFiles/BR/Refining_F3/infer_AR_with_two_days_model_ecFEV1_ecFEF2575.xlsx\",\n",
    "    [AR.name],\n",
    "    [\"Day\"],\n",
    "    # ).drop(columns=[\"Unnamed: 0\", HO2Sat.name, IA.name, HFEV1.name])\n",
    ").drop(columns=[HO2Sat.name, IA.name, HFEV1.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_elapsed_for_offset(df_for_ID, idx_offset=1):\n",
    "    \"\"\"\n",
    "    Links each recording with a previous recording that is idx_offset indices before it\n",
    "    For idx_offset = 1, consecutive recordings are linked\n",
    "    \"\"\"\n",
    "    df_for_ID = df_for_ID.copy()\n",
    "\n",
    "    def calc_days_elapsed(curr, prev):\n",
    "        \"\"\"\n",
    "        Takes in dates in format\n",
    "        \"\"\"\n",
    "        if prev == None:\n",
    "            return None\n",
    "        return (curr - prev).days\n",
    "        # return (curr - prev).total_seconds() / 3600 / 24\n",
    "\n",
    "    df_for_ID[\"Prev date\"] = df_for_ID.shift(idx_offset)[\"Day\"]\n",
    "    s_days_elapsed = df_for_ID.apply(\n",
    "        lambda x: calc_days_elapsed(x[\"Day\"], x[\"Prev date\"]), axis=1\n",
    "    )\n",
    "\n",
    "    return s_days_elapsed\n",
    "\n",
    "\n",
    "def get_days_elapsed_and_AR_mean_shift(df_for_ID, idx_offset=1):\n",
    "    df_for_ID = df_for_ID.copy()\n",
    "    df_for_ID[\"Days elapsed\"] = get_days_elapsed_for_offset(df_for_ID, idx_offset)\n",
    "\n",
    "    df_for_ID[\"AR mean\"] = df_for_ID.apply(lambda x: AR.get_mean(x[AR.name]), axis=1)\n",
    "    # df_for_ID['AR skewness'] = df_for_ID.apply(lambda x: AR.get_skewness(x[AR.name]), axis=1)\n",
    "\n",
    "    df_for_ID[\"Prev AR mean\"] = df_for_ID.shift(idx_offset)[\"AR mean\"]\n",
    "    # df_for_ID['Prev AR skewness'] = df_for_ID.shift(idx_offset)['AR skewness']\n",
    "\n",
    "    df_for_ID[\"AR mean shift\"] = df_for_ID[\"AR mean\"] - df_for_ID[\"Prev AR mean\"]\n",
    "    # df_for_ID['AR skewness shift'] = df_for_ID['AR skewness'] - df_for_ID['Prev AR skewness']\n",
    "\n",
    "    return df_for_ID[[\"ID\", \"Day\", \"Days elapsed\", \"AR mean shift\"]]\n",
    "    # return df_for_ID[['ID', 'Day', 'Days elapsed', 'AR mean shift', 'AR skewness shift']]\n",
    "\n",
    "\n",
    "def generate_AR_change_sample(df_samples_for_ID, idx_offset=1):\n",
    "    \"\"\"\n",
    "    1. Sample from AR1 and AR2\n",
    "    2. Compute the change in AR and save it\n",
    "    3. Repeat 500 times for this ID, then aggregate the results across IDs\n",
    "    500*300 ID = 150,000 samples, can do more if needed\n",
    "    \"\"\"\n",
    "    df_samples_for_ID = df_samples_for_ID.copy()\n",
    "\n",
    "    df_samples_for_ID[\"Days elapsed\"] = get_days_elapsed_for_offset(\n",
    "        df_samples_for_ID, idx_offset\n",
    "    )\n",
    "\n",
    "    df_samples_for_ID[\"Prev AR samples\"] = df_samples_for_ID.shift(idx_offset)[\n",
    "        \"AR samples\"\n",
    "    ]\n",
    "\n",
    "    # Remove entries at the boundaries that have no previous recordings after applying the offset\n",
    "    df_samples_for_ID = df_samples_for_ID.dropna(subset=[\"Prev AR samples\"])\n",
    "\n",
    "    df_samples_for_ID[\"AR samples shift\"] = df_samples_for_ID.apply(\n",
    "        lambda row: row[\"AR samples\"] - row[\"Prev AR samples\"], axis=1\n",
    "    )\n",
    "    df_samples_for_ID = df_samples_for_ID.explode(\"AR samples shift\")\n",
    "\n",
    "    return df_samples_for_ID[[\"ID\", \"Day\", \"Days elapsed\", \"AR samples shift\"]]\n",
    "\n",
    "\n",
    "# out = df.groupby('ID').apply(get_days_elapsed_and_AR_mean_shift).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute day elapsed between two consecutive entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.merge(\n",
    "    df.groupby(\"ID\").apply(get_days_elapsed_and_AR_mean_shift).reset_index(drop=True),\n",
    "    on=[\"ID\", \"Day\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Days elapsed\"] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the prev day is indeed correct)\n",
    "df1.iloc[2295:2297]\n",
    "# Count number of None\n",
    "print(df1[\"Days elapsed\"].isna().sum())\n",
    "# Count number if ids\n",
    "print(df1[\"ID\"].nunique())\n",
    "# They should be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"AR mean shift\"] > 20]\n",
    "df1.iloc[2537:2539]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse time between two consecutive entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = df1[\"Days elapsed\"].value_counts()\n",
    "# 1/3 of the consecutive indices are more than 1 day apart (~10k entries)\n",
    "# 97% of the entries are less than 5 days apart from the previous entry\n",
    "# For the CPT, I'll take 1, 2, 3, 4, 5 days apart, then avg 6-50 -> this last up to the max days diff\n",
    "\n",
    "\n",
    "# Plot the histogram with vc index and vc values\n",
    "fig = px.bar(x=vc.index, y=vc.values / sum(vc.values) * 100)\n",
    "# Set x axis label to day to day difference\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Number of days between two consecutive entries\",\n",
    "    range=[0, 30],\n",
    "    tickvals=list(range(0, 31, 1)),\n",
    ")\n",
    "# Set y axis label to percentage\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Percentage of total entries (%)\", tickvals=[2] + list(range(0, 55, 5))\n",
    ")\n",
    "\n",
    "title = \"Distribution of the time between two measurements\"\n",
    "# Set title\n",
    "fig.update_layout(title=title, width=800, height=350, font=dict(size=10))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# # Save figure\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study per ID\n",
    "# Get idx at which the days elapsed is more than 3\n",
    "\n",
    "df1[df1[\"Days elapsed\"] > 3].index\n",
    "\n",
    "\n",
    "def get_idx_more_than_n_days_elapsed(df, n=3):\n",
    "    df = df.reset_index()\n",
    "    n_days_total = df.shape[0]\n",
    "    df_tmp = df[df[\"Days elapsed\"] > n]\n",
    "    if df_tmp.empty:\n",
    "        return n_days_total, n_days_total\n",
    "    n_days_consec = df_tmp.index[0]\n",
    "    return n_days_consec, n_days_total\n",
    "\n",
    "\n",
    "s_n_entries_to_break = (\n",
    "    df1.groupby(\"ID\")\n",
    "    .apply(lambda x: get_idx_more_than_n_days_elapsed(x, 3))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "s_n_entries_to_break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.ID == \"101\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute shift in airway resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max offset between entries will be equal to the max number of days elapsed in the model, to maximise the contributing data\n",
    "max_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AR norm\"] = df.apply(lambda row: row[AR.name] / sum(row[AR.name]), axis=1)\n",
    "df[\"AR samples\"] = df.apply(lambda row: AR.sample(n=50, p=row[\"AR norm\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build aggregate df of shift in AR for different offsets\n",
    "\n",
    "df_mixed_offset = pd.DataFrame()\n",
    "\n",
    "for n_idx_offset in range(1, max_offset + 1):\n",
    "    print(\"offset\", n_idx_offset)\n",
    "    df_offset = (\n",
    "        df.groupby(\"ID\")\n",
    "        .apply(lambda df_for_ID: generate_AR_change_sample(df_for_ID, n_idx_offset))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_offset[\"Offset\"] = n_idx_offset\n",
    "    # Remove nan\n",
    "    df_offset = df_offset.dropna()\n",
    "\n",
    "    # Add to mix offset\n",
    "    df_mixed_offset = pd.concat([df_mixed_offset, df_offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mixed_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the shift in airway resistance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:* Checking for same day measurements *\n"
     ]
    }
   ],
   "source": [
    "# The previous methods was erroneous because it was taking to independent samples from AR day 1 and AR day 2.\n",
    "# We have to generate joint samples. Given a two days, model,\n",
    "# 1/ Infer AR1 using the two consecutive days model\n",
    "# 2/ Sample from AR1\n",
    "# 3/ Infer HFEV1 and HO2Sat using the sampled AR1 and the observations\n",
    "# 4/ Infer AR2 using the inferred HFEV1 and HO2Sat\n",
    "# 5/ Sample from AR2\n",
    "df_obs = bd.load_meas_from_excel(\"BR_O2_FEV1_FEF2575_conservative_smoothing_with_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_jointly_from_AR(df_two_days):\n",
    "    df_two_days = df_two_days.copy()\n",
    "    height = df_two_days.loc[0, \"Height\"]\n",
    "    age = df_two_days.loc[0, \"Age\"]\n",
    "    sex = df_two_days.loc[0, \"Sex\"]\n",
    "    id = df_two_days.loc[0, \"ID\"]\n",
    "    (\n",
    "        model,\n",
    "        inf_alg,\n",
    "        HFEV1,\n",
    "        ecFEV1,\n",
    "        AR,\n",
    "        HO2Sat,\n",
    "        O2SatFFA,\n",
    "        IA,\n",
    "        UO2Sat,\n",
    "        O2Sat,\n",
    "        ecFEF2575prctecFEV1,\n",
    "    ) = mb.o2sat_fev1_fef2575_point_in_time_model_shared_healthy_vars(height, age, sex)\n",
    "\n",
    "    # Set variables parametrisation\n",
    "    key_hfev1 = f\"['{ecFEV1.name}', '{HFEV1.name}', '{AR.name}'] -> {HFEV1.name}\"\n",
    "    key_ho2sat = f\"['{O2SatFFA.name}', '{HO2Sat.name}', '{AR.name}'] -> {HO2Sat.name}\"\n",
    "    HFEV1.set_factor_node_key(key_hfev1)\n",
    "    HO2Sat.set_factor_node_key(key_ho2sat)\n",
    "\n",
    "    # 1/ Infer AR1 using the two consecutive days model\n",
    "    print(\"Infer AR1\")\n",
    "    df_res_final_epoch1, _, _ = slicing.query_back_and_forth_across_days_joint_samples(\n",
    "        df_two_days,\n",
    "        inf_alg,\n",
    "        [HFEV1, HO2Sat],\n",
    "        [AR],\n",
    "        [ecFEV1.name, ecFEF2575prctecFEV1.name],\n",
    "        1e-8,\n",
    "        days_specific_evidence=[],\n",
    "        max_passes=5,\n",
    "    )\n",
    "\n",
    "    # 2/ Sample from AR1\n",
    "    print(\"Sample from AR1\")\n",
    "    ar_day1_dist = df_res_final_epoch1.loc[0, AR.name]\n",
    "    [ar_day1_sample] = AR.sample(n=1, p=ar_day1_dist)\n",
    "    idx_ar = AR.get_bin_for_value(ar_day1_sample)[1]\n",
    "\n",
    "    df_two_days[\"AR\"] = [ar_day1_sample, -1]\n",
    "    df_two_days[f\"idx {AR.name}\"] = [idx_ar, 1000000]\n",
    "\n",
    "    # 3/ Infer AR2 using with sampled AR1 as evidence specific to day 1\n",
    "    print(\"Infer AR2\")\n",
    "    days_specific_evidence = [(AR.name, [df_res_final_epoch1.loc[0, \"Day\"]])]\n",
    "\n",
    "    df_res_final_epoch2, _, _ = slicing.query_back_and_forth_across_days_joint_samples(\n",
    "        df_two_days,\n",
    "        inf_alg,\n",
    "        [HFEV1, HO2Sat],\n",
    "        [AR],\n",
    "        [ecFEV1.name, ecFEF2575prctecFEV1.name],\n",
    "        1e-8,\n",
    "        days_specific_evidence,\n",
    "        max_passes=5,\n",
    "        debug=True\n",
    "    )\n",
    "    ar_day2_dist = df_res_final_epoch2.loc[1, AR.name]\n",
    "\n",
    "    # 5/ Sample from AR2\n",
    "    [ar_day2_sample] = AR.sample(n=1, p=ar_day2_dist)\n",
    "\n",
    "    return ar_day1_sample, ar_day2_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer AR1\n",
      "Pass 1 - Posteriors' diff for Healthy FEV1 (L): 1.1913058316276834\n",
      "Pass 1 - Posteriors' diff for Healthy O2 saturation (%): 1.4626076334969156\n",
      "Pass 2 - Posteriors' diff for Healthy FEV1 (L): 2.0304610590437058e-16\n",
      "Pass 2 - Posteriors' diff for Healthy O2 saturation (%): 2.456605818644018e-16\n",
      "Pass 3 - Posteriors' diff for Healthy FEV1 (L): 1.919521721142268e-16\n",
      "Pass 3 - Posteriors' diff for Healthy O2 saturation (%): 1.7800228000639186e-16\n",
      "Sample from AR1\n",
      "Infer AR2\n",
      "Adding Airway resistance (%) to the evidence list for 2019-01-25\n",
      "Removing Airway resistance (%) from the variables list for 2019-01-25\n",
      "Date 2019-01-25 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 20, 'Airway resistance (%)': 29} and virtual evidence: [('Healthy FEV1 (L)', array([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Date 2019-01-26 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)', 'Airway resistance (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 21} and virtual evidence: [('Healthy FEV1 (L)', array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00063   , 0.08180184, 0.21608287, 0.31229096,\n",
      "       0.25049613, 0.12344063, 0.01525757, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Pass 1 - Posteriors' diff for Healthy FEV1 (L): 1.8790607275927276\n",
      "Pass 1 - Posteriors' diff for Healthy O2 saturation (%): 1.4626076334969143\n",
      "Date 2019-01-26 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)', 'Airway resistance (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 21} and virtual evidence: [('Healthy FEV1 (L)', array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00063   , 0.08180184, 0.21608287, 0.31229096,\n",
      "       0.25049613, 0.12344063, 0.01525757, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Adding Airway resistance (%) to the evidence list for 2019-01-25\n",
      "Removing Airway resistance (%) from the variables list for 2019-01-25\n",
      "Date 2019-01-25 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 20, 'Airway resistance (%)': 29} and virtual evidence: [('Healthy FEV1 (L)', array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.14269468, 0.14249422, 0.14338417, 0.14346262,\n",
      "       0.14257299, 0.1427514 , 0.14263993, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Pass 2 - Posteriors' diff for Healthy FEV1 (L): 1.1806961658367143e-16\n",
      "Pass 2 - Posteriors' diff for Healthy O2 saturation (%): 2.440345335614097e-16\n",
      "Alg. converged - All diffs are below 1e-08, running another epoch to get all posteriors\n",
      "Adding Airway resistance (%) to the evidence list for 2019-01-25\n",
      "Removing Airway resistance (%) from the variables list for 2019-01-25\n",
      "Date 2019-01-25 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 20, 'Airway resistance (%)': 29} and virtual evidence: [('Healthy FEV1 (L)', array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.14269468, 0.14249422, 0.14338417, 0.14346262,\n",
      "       0.14257299, 0.1427514 , 0.14263993, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Date 2019-01-26 - Querying all variables: ['Healthy FEV1 (L)', 'Healthy O2 saturation (%)', 'Airway resistance (%)'] with evidence: {'ecFEV1 (L)': 26, 'ecFEF25-75 % ecFEV1 (%)': 21} and virtual evidence: [('Healthy FEV1 (L)', array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.00063   , 0.08180184, 0.21608287, 0.31229096,\n",
      "       0.25049613, 0.12344063, 0.01525757, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ])), ('Healthy O2 saturation (%)', array([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
      "       0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]))]\n",
      "Pass 3 - Posteriors' diff for Healthy FEV1 (L): 1.4246416546459528e-16\n",
      "Pass 3 - Posteriors' diff for Healthy O2 saturation (%): 1.3454954275220935e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58.69140642637186, 56.2536282838788)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_ID = df_obs[df_obs[\"ID\"] == \"101\"]\n",
    "\n",
    "df_two_days = df_for_ID.iloc[0:2]\n",
    "\n",
    "sample_jointly_from_AR(df_two_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with days elapsed on x axis and AR diff on y axis, using px\n",
    "y_col = \"AR mean shift\"\n",
    "y_col = \"AR samples shift\"\n",
    "# y_col = 'AR skewness shift'\n",
    "fig = px.scatter(df_mixed_offset, x=\"Days elapsed\", y=y_col, color=\"ID\")\n",
    "# Set x axis range to 0-100\n",
    "fig.update_xaxes(range=[0, 200])\n",
    "fig.update_xaxes(range=[0, 50], title=\"Number of days elapsed\")\n",
    "# Add more y axi tick vals\n",
    "fig.update_yaxes(title=\"Mean airway resistance shift (%)\")\n",
    "# Reduce marker size\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "title = f\"How much does the airway resistance change in n days (1- {max_offset} idx offset)? - samples\"\n",
    "fig.update_layout(\n",
    "    title=title, width=800, height=400, font=dict(size=10), showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see the distribution of AR diffs for each day elapsed\n",
    "from scipy.stats import norm\n",
    "\n",
    "y_col = \"AR mean shift\"\n",
    "y_col = \"AR samples shift\"\n",
    "\n",
    "fig = make_subplots(rows=6, cols=1, shared_xaxes=True)\n",
    "xbin_size = 0.2\n",
    "# xbin_size = 1\n",
    "xbin_absolute_span = 50\n",
    "# xbin_absolute_span = 10\n",
    "xbins = dict(\n",
    "    start=-xbin_absolute_span - 0.5, end=xbin_absolute_span + 0.5, size=xbin_size\n",
    ")\n",
    "\n",
    "\n",
    "def add_plot_for_n_days_elapsed(n_days_elapsed, row):\n",
    "    df_tmp = df_mixed_offset[df_mixed_offset[\"Days elapsed\"] == n_days_elapsed]\n",
    "    print(n_days_elapsed, df_tmp.shape)\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df_tmp[y_col],\n",
    "            xbins=xbins,\n",
    "            histnorm=\"probability\",\n",
    "            name=(\n",
    "                f\"{n_days_elapsed} day elapsed\"\n",
    "                if n_days_elapsed == 1\n",
    "                else f\"{n_days_elapsed} days elapsed\"\n",
    "            ),\n",
    "        ),\n",
    "        row=row,\n",
    "        col=1,\n",
    "    )\n",
    "    return df_tmp\n",
    "    # Model the data by a normal distribution\n",
    "    # mean = df_tmp[y_col].mean()\n",
    "    # std = df_tmp[y_col].std()\n",
    "    # x = list(range(-10, 11))\n",
    "    # y = norm.pdf(x, loc=mean, scale=std)\n",
    "    # Add trace\n",
    "    # fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=f\"Normal distribution for {offset} days offset\"), row=row, col=1)\n",
    "\n",
    "\n",
    "# for offset in range(1, 51):\n",
    "#     add_plot_for_n_days_elapsed(offset, offset)\n",
    "\n",
    "df_1_DE = add_plot_for_n_days_elapsed(1, 1)\n",
    "add_plot_for_n_days_elapsed(2, 2)\n",
    "add_plot_for_n_days_elapsed(8, 3)\n",
    "add_plot_for_n_days_elapsed(14, 4)\n",
    "add_plot_for_n_days_elapsed(20, 5)\n",
    "add_plot_for_n_days_elapsed(50, 6)\n",
    "\n",
    "# Set y axis range to 0, 0.6\n",
    "# fig.update_yaxes(range=[0, 0.58])\n",
    "# Set x axis label\n",
    "fig.update_xaxes(title_text=\"Shift in mean airway resistance (%)\", row=6, col=1)\n",
    "# fig.update_xaxes(title_text='Change in skewness of airway resistance (%)', row=6, col=1)\n",
    "# Add x axis tick vals\n",
    "# fig.update_xaxes(tickvals=np.arange(-10, 11, 1), row=6, col=1)\n",
    "# fig.update_xaxes(tickvals=np.arange(-50, 55, 5), row=6, col=1)\n",
    "# Update layout\n",
    "# title = f\"Shift in airway resistance for different time periods elapsed (bin_width = {xbin_size}%, bin_span = {xbin_absolute_span})\"\n",
    "title = f\"Shift in airway resistance - O2sat, ecFEV1\"\n",
    "# fig.update_layout(height=2600, width=1000, title=title)\n",
    "fig.update_layout(height=600, width=1000, title=title)\n",
    "\n",
    "# Keep y axis lower\n",
    "fig.update_yaxes(range=[0, 0.01])\n",
    "#\n",
    "# Save image\n",
    "fig.write_image(\n",
    "    f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\n",
    "    f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title} - samples.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building P(AR_next | days_elapsed, AR_prev)\n",
    "# import src.models.helpers as mh\n",
    "import numpy as np\n",
    "import src.modelling_ar.ar as model_ar\n",
    "\n",
    "AR1 = mh.VariableNode(\n",
    "    \"Airway resistance day 1 (%)\", 0, 90, 2, prior={\"type\": \"uniform\"}\n",
    ")\n",
    "AR2 = mh.VariableNode(\n",
    "    \"Airway resistance day 2 (%)\", 0, 90, 2, prior={\"type\": \"uniform\"}\n",
    ")\n",
    "# Set the max number of days elapsed to max offset in order to have as much data as possible per number of days elapsed\n",
    "DE = mh.DiscreteVariableNode(\"Days elapsed\", 1, max_offset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cpt(\n",
    "    AR_next_day: mh.VariableNode,\n",
    "    AR_curr_day: mh.VariableNode,\n",
    "    DE: mh.DiscreteVariableNode,\n",
    "    shift_p,\n",
    "    shift_val,\n",
    "    tol=1e-6,\n",
    "    debug=False,\n",
    "):\n",
    "    cpt = np.zeros([AR_next_day.card, AR_curr_day.card, DE.card])\n",
    "\n",
    "    for i, de in enumerate(DE.values):\n",
    "        # For each shift value, get the mapping AR -> AR_next_day for each shifted bin in AR\n",
    "        # Weight the result by the probability of that shift\n",
    "        # Add it to the CPT for this day\n",
    "        for s in range(len(shift_val)):\n",
    "            if debug:\n",
    "                print(f\"Computing CPT for days elapsed={de}, shift={shift_val[s]}\")\n",
    "            # Summing over the columns of the cpt returned by calc_cpt_X_plus_k should give 1, except at the boundaries\n",
    "            # Since we weight the 1s by a probability of shift that also sums to one, the sum of the cpt should be 1 (except at the boundaries, see below)\n",
    "            cpt_contrib = calc_cpt_X_plus_k(\n",
    "                AR_curr_day,\n",
    "                AR_next_day,\n",
    "                shift_val[s],\n",
    "                tol=tol,\n",
    "                debug=debug,\n",
    "            )\n",
    "            # If has nan\n",
    "            if (np.isnan(cpt_contrib) == True).any():\n",
    "                print(\"issue with cpt contribution\")\n",
    "                print(cpt_contrib)\n",
    "            cpt[:, :, i] += shift_p[i, s] * cpt_contrib\n",
    "        # Normalise the CPT along axis 0 (AR_next_day)\n",
    "        total = np.sum(cpt[:, :, i], axis=0)\n",
    "        if (np.isnan(total) == False).all():\n",
    "            print(cpt[:, :, i])\n",
    "        print(\n",
    "            f\"Sum along axis 0 before normalisation: np.sum(cpt[:, :, {i}], axis=0) = {total}\"\n",
    "        )\n",
    "        cpt[:, :, i] /= total\n",
    "\n",
    "        # Check that the sum of probabilities is 1\n",
    "        total = np.sum(cpt[:, :, i], axis=0)\n",
    "        assert (\n",
    "            abs(total - 1) < tol\n",
    "        ).all(), f\"The sum of the probabilities should be 1, got sum(cpt)={total}])\"\n",
    "    return cpt\n",
    "\n",
    "\n",
    "def calc_cpt_X_plus_k(\n",
    "    Z: mh.VariableNode,\n",
    "    X: mh.VariableNode,\n",
    "    k,\n",
    "    tol=1e-6,\n",
    "    debug=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the CPT for P(Z|X, Y), when Z is shifted from X by a constant value k\n",
    "    Z = X + k\n",
    "    X: parent variable\n",
    "    Z: child variable\n",
    "    k: constant, positive or negative\n",
    "\n",
    "    We compute the CPT with a shift and conquer method:\n",
    "    1) Start with a CPT zeroed out probabilities\n",
    "    2) Shift all X bin intervals by the drop amount\n",
    "    3) For each shifted X bin, spread the X bin evenly onto the overlapping Z bins\n",
    "    4) Normalise the CPT\n",
    "\n",
    "    This allows the function to be agnostic of how X and Z are binned.\n",
    "\n",
    "    - What happens when the function is shifted outside the boundary? -> Raise an error as it shouldn't happen by how the model is built\n",
    "    \"\"\"\n",
    "    nbinsX = len(X.bins)\n",
    "    nbinsZ = len(Z.bins)\n",
    "\n",
    "    cpt = np.zeros([nbinsZ, nbinsX])\n",
    "\n",
    "    for i in range(nbinsX):\n",
    "        shifted_X_bin_low = X.bins[i] + k\n",
    "        shifted_X_bin_up = (X.bins[i] + X.bin_width) + k\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"Shifting X bin {i} from [{X.bins[i]};{X.bins[i]+X.bin_width}) to [{shifted_X_bin_low};{shifted_X_bin_up}), shift amount={k}%\"\n",
    "            )\n",
    "        # If the shifted bin is outside the boundaries of Z, continue:\n",
    "        if (\n",
    "            shifted_X_bin_low >= (Z.bins[-1] + Z.bin_width)\n",
    "            or shifted_X_bin_up <= Z.bins[0]\n",
    "        ):\n",
    "            if debug:\n",
    "                print(\n",
    "                    f\"Shift outside boundaries of Z.bins=[{Z.bins[0]};{Z.bins[-1] + Z.bin_width})\"\n",
    "                )\n",
    "            continue\n",
    "        # Handle the case where the shifted bin is partially outside the boundaries\n",
    "        # Adjust the boundaries of the shifted bin to be within the boundaries of Z\n",
    "        if shifted_X_bin_low < Z.bins[0]:\n",
    "            if debug:\n",
    "                print(\"Shift partially outside boundaries, adjusting lower boundary\")\n",
    "            shifted_X_bin_low = Z.bins[0]\n",
    "        if shifted_X_bin_up > Z.bins[-1] + Z.bin_width:\n",
    "            if debug:\n",
    "                print(\"Shift partially outside boundaries, adjusting upper boundary\")\n",
    "            shifted_X_bin_up = Z.bins[-1] + Z.bin_width\n",
    "\n",
    "        bin_contribution = mh.get_bin_contribution_to_cpt(\n",
    "            [shifted_X_bin_low, shifted_X_bin_up], Z.bins, debug=debug\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"i={i}/{nbinsX-1}, z={bin_contribution}\")\n",
    "        # There is just one bin contribution to the CPT\n",
    "        cpt[:, i] = bin_contribution\n",
    "\n",
    "    sum_over_x = np.sum(cpt, axis=0)\n",
    "    if debug:\n",
    "        print(f\"Results before normalisation sum(cpt)={sum_over_x}\")\n",
    "\n",
    "    # IMPORTANT: there is no boundary check in this function. This allows to have no conditional probability distribution for certain bins of AR2, which are not compatible with the amount of shift applied to AR1\n",
    "    # Therefore either the sum of probabilities is 0 or 1 for each bin of AR2, summed over AR1\n",
    "    for i in range(nbinsZ):\n",
    "        if sum_over_x[i] == 0:\n",
    "            if debug:\n",
    "                print(f\"Sum of probabilities is 0 for bin {i}, skipping normalisation\")\n",
    "            continue\n",
    "        cpt[i, :] /= sum_over_x[i]\n",
    "        assert (\n",
    "            abs(sum_over_x[i] - 1) < tol\n",
    "        ).all(), f\"The sum of the probabilities should be 1, got sum(cpt[i, :])={sum_over_x[i]}])\"\n",
    "\n",
    "    return cpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the shift distributions\n",
    "size = 0.2\n",
    "shift_min = -20\n",
    "shift_max = 20\n",
    "shift_val = np.arange(shift_min, shift_max + size / 2, 0.2)\n",
    "shift_p = np.empty((max_offset, len(shift_val)))\n",
    "\n",
    "# Check identity matrix if shift is 0\n",
    "# cpt_point_mass = np.zeros(len(shift_val))\n",
    "# cpt_point_mass[100] = 1\n",
    "\n",
    "for i, de in enumerate(DE.values):\n",
    "    print(\"days elapsed: \", de)\n",
    "    shift = df_mixed_offset[df_mixed_offset[\"Days elapsed\"] == de][\"AR samples shift\"]\n",
    "\n",
    "    # Bin up the mean shift series into bins starting at -5 and ending at 5, with bin size 1\n",
    "    shift_p[i, :] = np.histogram(\n",
    "        shift,\n",
    "        bins=np.arange(shift_min - size / 2, shift_max + size, size),\n",
    "        density=True,\n",
    "    )[0]\n",
    "    # shift_p[i, :] = cpt_point_mass\n",
    "\n",
    "print(\"shift probability shape: \", shift_p.shape)\n",
    "print(\"shift_val: \", shift_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = calc_cpt(AR2, AR1, DE, shift_p, shift_val, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test uniform shift distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: With the completely uniform CPT, each AR1 bin contributes equally to all AR2 bins, hence the output is uniform over AR2\n",
    "\n",
    "p_ar2 = np.ones(AR2.card) / AR2.card\n",
    "# Repeat p_ar2 on each column AR1.card times\n",
    "cpt_uni = np.repeat(p_ar2[:, np.newaxis], AR1.card, axis=1)\n",
    "# Repeat this cpt DE.card times\n",
    "cpt_uni = np.repeat(cpt_uni[:, :, np.newaxis], DE.card, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P of AR2 is uniform over 5 bins centered on the bin of AR1\n",
    "cpt_ar2_ar1 = np.zeros([AR2.card, AR1.card])\n",
    "# Add padding left and right\n",
    "padding = 15\n",
    "for i in range(AR1.card):\n",
    "    if i <= padding:\n",
    "        low = 0\n",
    "    else:\n",
    "        low = i - padding\n",
    "    if AR2.card - 1 <= (i + padding):\n",
    "        up = AR2.card - 1\n",
    "    else:\n",
    "        up = i + padding\n",
    "\n",
    "    idx_range = list(range(low, up + 1))\n",
    "    cpt_ar2_ar1[idx_range, i] = 1 / len(idx_range)\n",
    "\n",
    "cpt_thick_uni = np.repeat(cpt_ar2_ar1[:, :, np.newaxis], DE.card, axis=2)\n",
    "cpt_thick_uni.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CPT relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.inference.helpers as ih\n",
    "\n",
    "# cpt = cpt_uni\n",
    "\n",
    "\n",
    "def compare_ARs_for_one_entry(idx):\n",
    "    title = f\"P(AR_next | AR_prev, days_elapsed) for diffent days elapsed (idx {idx}) - samples\"\n",
    "    fig = make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    ih.plot_histogram(\n",
    "        fig, AR1, df.loc[idx, AR.name], AR1.a, AR1.b, 1, 1, name=\"AR day 1\", annot=False\n",
    "    )\n",
    "    AR_next_day_p = np.matmul(cpt[:, :, 0], df.loc[idx, AR.name])\n",
    "    ih.plot_histogram(\n",
    "        fig,\n",
    "        AR2,\n",
    "        AR_next_day_p,\n",
    "        AR2.a,\n",
    "        AR2.b,\n",
    "        1,\n",
    "        1,\n",
    "        name=\"AR day 2, days elapsed=1\",\n",
    "        annot=False,\n",
    "    )\n",
    "    # AR_next_day_p = np.matmul(cpt[:, :, 2], df.loc[idx, AR.name])\n",
    "    # ih.plot_histogram(\n",
    "    #     fig,\n",
    "    #     AR2,\n",
    "    #     AR_next_day_p,\n",
    "    #     AR2.a,\n",
    "    #     AR2.b,\n",
    "    #     1,\n",
    "    #     1,\n",
    "    #     name=\"AR day 2, days elapsed=3\",\n",
    "    #     annot=False,\n",
    "    # )\n",
    "    # Add x axis title\n",
    "    fig.update_xaxes(title_text=\"Airway resistance (%)\", row=1, col=1)\n",
    "    # Reduce figure height\n",
    "    fig.update_layout(height=200, width=1000, title=title, font=dict(size=10))\n",
    "    # remove marings\n",
    "    fig.update_layout(margin=dict(l=2, r=2, t=30, b=2))\n",
    "    fig.show()\n",
    "    # Save figure\n",
    "    # fig.write_image(\n",
    "    #     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "    # )\n",
    "\n",
    "\n",
    "compare_ARs_for_one_entry(20000)\n",
    "# compare_ARs_for_one_entry(21000)\n",
    "compare_ARs_for_one_entry(1000)\n",
    "compare_ARs_for_one_entry(4400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = 3\n",
    "fig, title = cpth.plot_2d_cpt(cpt_thick_uni[:, :, de - 1], AR2, AR1, 3000, invert=False)\n",
    "# Update font\n",
    "title = title + f\", {de} days elapsed, shift span [{shift_min};{shift_max}] - samples\"\n",
    "fig.update_layout(font=dict(size=7), title=title)\n",
    "fig.show()\n",
    "\n",
    "# Save figure\n",
    "# fig.write_image(\n",
    "#     f\"{dh.get_path_to_main()}/PlotsBreathe/Interconnecting_ARs_entries/{title}.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cpt\n",
    "cpth.save_cpt([AR2, AR1, DE], cpt, suffix=f\"_shift_span_[{shift_min};{shift_max}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the shift per bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR.midbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df1.copy()\n",
    "\n",
    "for i, row in df_exploded[0:10].iterrows():\n",
    "    row = pd.DataFrame(data=row[AR.name])\n",
    "    df_exploded = pd.concat([df_exploded, row], axis=1)\n",
    "\n",
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
